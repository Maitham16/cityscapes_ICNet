{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CityscapesTestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(os.path.join(root_dir, \"leftImg8bit\", \"test\")) for f in filenames if 'leftImg8bit.png' in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.images[index]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "test_dataset = CityscapesTestDataset('/home/maith/Desktop/cityscapes', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "class PyramidPoolingModule(nn.Module):\n",
    "    def __init__(self, in_channels, pool_sizes):\n",
    "        super(PyramidPoolingModule, self).__init__()\n",
    "        self.pools = [nn.AdaptiveAvgPool2d(output_size=size) for size in pool_sizes]\n",
    "        self.conv_blocks = nn.ModuleList([nn.Conv2d(in_channels, 512, 1) for _ in pool_sizes])\n",
    "        self.batch_norms = nn.ModuleList([nn.BatchNorm2d(512) for _ in pool_sizes])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for pool, conv, bn in zip(self.pools, self.conv_blocks, self.batch_norms):\n",
    "            pooled = pool(x)\n",
    "            convolved = conv(pooled)\n",
    "            upsampled = F.interpolate(convolved, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "            features.append(bn(upsampled))\n",
    "        return torch.cat(features, dim=1)\n",
    "\n",
    "class CascadeFeatureFusion(nn.Module):\n",
    "    def __init__(self, low_channels, high_channels, out_channels, num_classes):\n",
    "        super(CascadeFeatureFusion, self).__init__()\n",
    "        self.conv_low = nn.Conv2d(low_channels, out_channels, 3, padding=2, dilation=2)\n",
    "        self.conv_high = nn.Conv2d(high_channels, out_channels, 1)\n",
    "        self.conv_low_bn = nn.BatchNorm2d(out_channels)\n",
    "        self.conv_high_bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.class_conv = nn.Conv2d(out_channels, num_classes, 1)\n",
    "\n",
    "    def forward(self, low_res_input, high_res_input):\n",
    "        low_res = self.relu(self.conv_low_bn(self.conv_low(low_res_input)))\n",
    "        high_res = self.relu(self.conv_high_bn(self.conv_high(high_res_input)))\n",
    "        \n",
    "        high_res = F.interpolate(high_res, size=low_res.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        result = low_res + high_res\n",
    "        class_output = self.class_conv(result)\n",
    "        return result, class_output\n",
    "\n",
    "class ICNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ICNet, self).__init__()\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.ppm = PyramidPoolingModule(2048, [1, 2, 3, 6])\n",
    "        self.cff1 = CascadeFeatureFusion(1024, 4096, 256, num_classes)\n",
    "        self.cff2 = CascadeFeatureFusion(512, 256, 128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer1 = self.backbone[4](self.backbone[3](self.backbone[2](self.backbone[1](self.backbone[0](x)))))\n",
    "        layer2 = self.backbone[5](layer1)\n",
    "        layer3 = self.backbone[6](layer2)\n",
    "        layer4 = self.backbone[7](layer3)\n",
    "\n",
    "        ppm_output = self.ppm(layer4)\n",
    "\n",
    "        cff1_output, class_output1 = self.cff1(layer3, ppm_output)\n",
    "        cff2_output, class_output2 = self.cff2(layer2, cff1_output)\n",
    "\n",
    "        final_output = F.interpolate(class_output2, scale_factor=4, mode='bilinear', align_corners=False)  # Scale up to input image size\n",
    "\n",
    "        return final_output\n",
    "\n",
    "model = ICNet(num_classes=19)\n",
    "model.load_state_dict(torch.load('/home/maith/Desktop/cityscapes/trained_icnet_model_final.pth'))\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "CITYSCAPES_COLORS = np.array([\n",
    "    [128, 64, 128], [244, 35, 232], [70, 70, 70], [102, 102, 156], [190, 153, 153],\n",
    "    [153, 153, 153], [250, 170, 30], [220, 220, 0], [107, 142, 35], [152, 251, 152],\n",
    "    [70, 130, 180], [220, 20, 60], [255, 0, 0], [0, 0, 142], [0, 0, 70],\n",
    "    [0, 60, 100], [0, 80, 100], [0, 0, 230], [119, 11, 32]\n",
    "], dtype=np.uint8)\n",
    "\n",
    "def decode_segmap(image, num_classes=19):\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "    \n",
    "    for l in range(num_classes):\n",
    "        idx = image == l\n",
    "        r[idx] = CITYSCAPES_COLORS[l, 0]\n",
    "        g[idx] = CITYSCAPES_COLORS[l, 1]\n",
    "        b[idx] = CITYSCAPES_COLORS[l, 2]\n",
    "        \n",
    "    rgb = np.stack([r, g, b], axis=2)\n",
    "    return rgb\n",
    "\n",
    "class Denormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return tensor\n",
    "\n",
    "denormalize = Denormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def visualize_and_save_predictions(model, device, data_loader, num_images=5, save_dir='/home/maith/Desktop/cityscapes/predictions'):\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, paths in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = F.interpolate(outputs, size=(images.shape[2], images.shape[3]), mode='bilinear', align_corners=False)\n",
    "            pred_masks = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                if images_so_far >= num_images:\n",
    "                    return\n",
    "                \n",
    "                image = denormalize(images[i].cpu()).numpy().transpose(1, 2, 0)\n",
    "                pred_mask = pred_masks[i].cpu().numpy()\n",
    "                decoded_pred_mask = decode_segmap(pred_mask)\n",
    "                \n",
    "                pred_mask_image = Image.fromarray(decoded_pred_mask)\n",
    "                pred_mask_path = os.path.join(save_dir, f'predicted_mask_{os.path.basename(paths[i])}')\n",
    "                pred_mask_image.save(pred_mask_path)\n",
    "                \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(image)\n",
    "                plt.title(\"Input Image\")\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.imshow(decoded_pred_mask)\n",
    "                plt.title(\"Predicted Annotation\")\n",
    "                plt.savefig(os.path.join(save_dir, f'comparison_{os.path.basename(paths[i])}'))\n",
    "                plt.close()\n",
    "                \n",
    "                images_so_far += 1\n",
    "\n",
    "model.load_state_dict(torch.load('/home/maith/Desktop/cityscapes/trained_icnet_model_final.pth', map_location=device))\n",
    "visualize_and_save_predictions(model, device, test_loader, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy input tensor with the correct shape\n",
    "dummy_input = torch.randn(1, 3, 256, 512)\n",
    "\n",
    "# Move the dummy input to the same device as the model\n",
    "device = next(model.parameters()).device  # Gets the device of the model\n",
    "dummy_input = dummy_input.to(device)\n",
    "\n",
    "# Export the model to ONNX format without verbose logging\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\", input_names=['input'], output_names=['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enet_cityscapes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
