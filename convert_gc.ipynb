{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maith/Desktop/cityscapes/enet_cityscapes/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/maith/Desktop/cityscapes/enet_cityscapes/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class PyramidPoolingModule(nn.Module):\n",
    "    def __init__(self, in_channels, pool_sizes):\n",
    "        super(PyramidPoolingModule, self).__init__()\n",
    "        self.pools = [nn.AdaptiveAvgPool2d(output_size=size) for size in pool_sizes]\n",
    "        self.conv_blocks = nn.ModuleList([nn.Conv2d(in_channels, 512, 1) for _ in pool_sizes])\n",
    "        self.batch_norms = nn.ModuleList([nn.BatchNorm2d(512) for _ in pool_sizes])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for pool, conv, bn in zip(self.pools, self.conv_blocks, self.batch_norms):\n",
    "            pooled = pool(x)\n",
    "            convolved = conv(pooled)\n",
    "            upsampled = F.interpolate(convolved, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "            features.append(bn(upsampled))\n",
    "        return torch.cat(features, dim=1)\n",
    "\n",
    "class CascadeFeatureFusion(nn.Module):\n",
    "    def __init__(self, low_channels, high_channels, out_channels, num_classes):\n",
    "        super(CascadeFeatureFusion, self).__init__()\n",
    "        self.conv_low = nn.Conv2d(low_channels, out_channels, 3, padding=2, dilation=2)\n",
    "        self.conv_high = nn.Conv2d(high_channels, out_channels, 1)\n",
    "        self.conv_low_bn = nn.BatchNorm2d(out_channels)\n",
    "        self.conv_high_bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.class_conv = nn.Conv2d(out_channels, num_classes, 1)\n",
    "\n",
    "    def forward(self, low_res_input, high_res_input):\n",
    "        low_res = self.relu(self.conv_low_bn(self.conv_low(low_res_input)))\n",
    "        high_res = self.relu(self.conv_high_bn(self.conv_high(high_res_input)))\n",
    "        high_res = F.interpolate(high_res, size=low_res.shape[2:], mode='bilinear', align_corners=False)\n",
    "        result = low_res + high_res\n",
    "        class_output = self.class_conv(result)\n",
    "        return result, class_output\n",
    "\n",
    "class ICNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ICNet, self).__init__()\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.ppm = PyramidPoolingModule(2048, [1, 2, 3, 6])\n",
    "        self.cff1 = CascadeFeatureFusion(1024, 4096, 256, num_classes)\n",
    "        self.cff2 = CascadeFeatureFusion(512, 256, 128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer1 = self.backbone[4](self.backbone[3](self.backbone[2](self.backbone[1](self.backbone[0](x)))))\n",
    "        layer2 = self.backbone[5](layer1)\n",
    "        layer3 = self.backbone[6](layer2)\n",
    "        layer4 = self.backbone[7](layer3)\n",
    "        ppm_output = self.ppm(layer4)\n",
    "        cff1_output, class_output1 = self.cff1(layer3, ppm_output)\n",
    "        cff2_output, class_output2 = self.cff2(layer2, cff1_output)\n",
    "        final_output = F.interpolate(class_output2, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        return final_output\n",
    "\n",
    "# Initialize model and load weights\n",
    "model = ICNet(num_classes=19)\n",
    "model.load_state_dict(torch.load('trained_icnet_model_final.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been successfully converted to TensorRT.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch2trt\n",
    "from torchvision import models\n",
    "\n",
    "# Ensure your model is on the GPU\n",
    "model = ICNet(num_classes=19).cuda().eval()  # Add .cuda() here\n",
    "model.load_state_dict(torch.load('/home/maith/Desktop/cityscapes/trained_icnet_model_final.pth'))\n",
    "\n",
    "# Create dummy input data as a tensor on the GPU\n",
    "x = torch.randn(1, 3, 256, 512).cuda()  # Input tensor is already on GPU\n",
    "\n",
    "# Convert to TensorRT using torch2trt\n",
    "model_trt = torch2trt.torch2trt(model, [x])\n",
    "\n",
    "# Optionally, save the converted model\n",
    "torch.save(model_trt.state_dict(), 'model_trt.pth')\n",
    "\n",
    "print(\"Model has been successfully converted to TensorRT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original PyTorch model state dictionary has been saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch2trt\n",
    "from torchvision import models\n",
    "\n",
    "# Ensure your model is on the GPU\n",
    "model = ICNet(num_classes=19).cuda().eval()\n",
    "model.load_state_dict(torch.load('/home/maith/Desktop/cityscapes/trained_icnet_model_final.pth'))\n",
    "\n",
    "# Save the PyTorch model state dictionary\n",
    "torch.save(model.state_dict(), 'model_state_dict.pth')\n",
    "\n",
    "print(\"Original PyTorch model state dictionary has been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/16/2024-03:14:32] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "Output Shape: torch.Size([1, 19, 128, 256])\n",
      "Sample Output: tensor([[[-7.1987, -7.1987, -7.2587,  ..., -6.8243, -6.8004, -6.8004],\n",
      "         [-7.1987, -7.1987, -7.2587,  ..., -6.8243, -6.8004, -6.8004],\n",
      "         [-7.3054, -7.3054, -7.4263,  ..., -6.8824, -6.8196, -6.8196],\n",
      "         ...,\n",
      "         [-8.0553, -8.0553, -8.0198,  ..., -7.6188, -7.6388, -7.6388],\n",
      "         [-8.0771, -8.0771, -7.9855,  ..., -7.6263, -7.6839, -7.6839],\n",
      "         [-8.0771, -8.0771, -7.9855,  ..., -7.6263, -7.6839, -7.6839]],\n",
      "\n",
      "        [[-9.2225, -9.2225, -9.2125,  ..., -9.2795, -9.4608, -9.4608],\n",
      "         [-9.2225, -9.2225, -9.2125,  ..., -9.2795, -9.4608, -9.4608],\n",
      "         [-9.2586, -9.2586, -9.2564,  ..., -9.2289, -9.4335, -9.4335],\n",
      "         ...,\n",
      "         [-7.6792, -7.6792, -7.6215,  ..., -5.3374, -5.5047, -5.5047],\n",
      "         [-7.8279, -7.8279, -7.7514,  ..., -5.3520, -5.5160, -5.5160],\n",
      "         [-7.8279, -7.8279, -7.7514,  ..., -5.3520, -5.5160, -5.5160]],\n",
      "\n",
      "        [[-1.1190, -1.1190, -1.5772,  ..., -0.4929, -0.2627, -0.2627],\n",
      "         [-1.1190, -1.1190, -1.5772,  ..., -0.4929, -0.2627, -0.2627],\n",
      "         [-1.4134, -1.4134, -2.0605,  ..., -0.9175, -0.5675, -0.5675],\n",
      "         ...,\n",
      "         [-0.5294, -0.5294, -0.9304,  ..., -3.9711, -3.8179, -3.8179],\n",
      "         [-0.2752, -0.2752, -0.5558,  ..., -3.8053, -3.7355, -3.7355],\n",
      "         [-0.2752, -0.2752, -0.5558,  ..., -3.8053, -3.7355, -3.7355]],\n",
      "\n",
      "        [[ 2.3284,  2.3284,  1.9541,  ...,  2.7575,  3.0933,  3.0933],\n",
      "         [ 2.3284,  2.3284,  1.9541,  ...,  2.7575,  3.0933,  3.0933],\n",
      "         [ 1.9215,  1.9215,  0.9996,  ...,  1.9493,  2.6484,  2.6484],\n",
      "         ...,\n",
      "         [ 2.7752,  2.7752,  1.9104,  ..., -0.0774,  0.4334,  0.4334],\n",
      "         [ 3.2019,  3.2019,  2.6952,  ...,  0.3654,  0.6130,  0.6130],\n",
      "         [ 3.2019,  3.2019,  2.6952,  ...,  0.3654,  0.6130,  0.6130]],\n",
      "\n",
      "        [[-2.8544, -2.8544, -2.9839,  ..., -3.9419, -3.8644, -3.8644],\n",
      "         [-2.8544, -2.8544, -2.9839,  ..., -3.9419, -3.8644, -3.8644],\n",
      "         [-2.9078, -2.9078, -3.0098,  ..., -3.8942, -3.8350, -3.8350],\n",
      "         ...,\n",
      "         [-4.1191, -4.1191, -4.0826,  ..., -4.1973, -4.1629, -4.1629],\n",
      "         [-4.0406, -4.0406, -4.0000,  ..., -4.2023, -4.1767, -4.1767],\n",
      "         [-4.0406, -4.0406, -4.0000,  ..., -4.2023, -4.1767, -4.1767]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch2trt\n",
    "from torchvision import models\n",
    "\n",
    "# Ensure your model architecture is defined\n",
    "model = ICNet(num_classes=19).cuda().eval()\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('model_state_dict.pth'))\n",
    "\n",
    "# Create dummy input data as a tensor on the GPU\n",
    "x = torch.randn(1, 3, 256, 512).cuda()  # Adjust input size as necessary\n",
    "\n",
    "# Convert to TensorRT using torch2trt\n",
    "model_trt = torch2trt.torch2trt(model, [x])\n",
    "\n",
    "# Perform a forward pass with the same dummy input or new test data\n",
    "output = model_trt(x)\n",
    "print(\"Output Shape:\", output.shape)\n",
    "print(\"Sample Output:\", output[0, :5])  # Print part of the output to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([1, 19, 128, 256])\n",
      "Sample Output: tensor([[[ -6.6982,  -6.6982,  -6.7221,  ...,  -7.2160,  -7.2406,  -7.2406],\n",
      "         [ -6.6982,  -6.6982,  -6.7221,  ...,  -7.2160,  -7.2406,  -7.2406],\n",
      "         [ -6.7570,  -6.7570,  -6.8429,  ...,  -7.2853,  -7.2654,  -7.2654],\n",
      "         ...,\n",
      "         [ -7.2554,  -7.2554,  -7.3055,  ...,  -7.8772,  -7.8347,  -7.8347],\n",
      "         [ -7.2161,  -7.2161,  -7.2289,  ...,  -7.8565,  -7.8380,  -7.8380],\n",
      "         [ -7.2161,  -7.2161,  -7.2289,  ...,  -7.8565,  -7.8380,  -7.8380]],\n",
      "\n",
      "        [[ -9.1625,  -9.1625,  -9.0636,  ..., -10.2583, -10.4156, -10.4156],\n",
      "         [ -9.1625,  -9.1625,  -9.0636,  ..., -10.2583, -10.4156, -10.4156],\n",
      "         [ -9.1241,  -9.1241,  -9.0631,  ..., -10.1369, -10.3068, -10.3068],\n",
      "         ...,\n",
      "         [ -7.0837,  -7.0837,  -7.1369,  ...,  -4.9820,  -5.0064,  -5.0064],\n",
      "         [ -7.0995,  -7.0995,  -7.1609,  ...,  -4.9968,  -4.9958,  -4.9958],\n",
      "         [ -7.0995,  -7.0995,  -7.1609,  ...,  -4.9968,  -4.9958,  -4.9958]],\n",
      "\n",
      "        [[ -0.4379,  -0.4379,  -0.8438,  ...,  -1.6409,  -1.4922,  -1.4922],\n",
      "         [ -0.4379,  -0.4379,  -0.8438,  ...,  -1.6409,  -1.4922,  -1.4922],\n",
      "         [ -0.8432,  -0.8432,  -1.3954,  ...,  -2.1188,  -1.8566,  -1.8566],\n",
      "         ...,\n",
      "         [ -0.4442,  -0.4442,  -0.7975,  ...,  -3.5611,  -3.3915,  -3.3915],\n",
      "         [ -0.2707,  -0.2707,  -0.5089,  ...,  -3.3955,  -3.3377,  -3.3377],\n",
      "         [ -0.2707,  -0.2707,  -0.5089,  ...,  -3.3955,  -3.3377,  -3.3377]],\n",
      "\n",
      "        [[  2.2291,   2.2291,   1.9115,  ...,   2.5865,   2.8563,   2.8563],\n",
      "         [  2.2291,   2.2291,   1.9115,  ...,   2.5865,   2.8563,   2.8563],\n",
      "         [  1.7166,   1.7166,   0.9331,  ...,   1.6093,   2.2337,   2.2337],\n",
      "         ...,\n",
      "         [  2.7124,   2.7124,   1.9030,  ...,   0.5689,   1.0351,   1.0351],\n",
      "         [  3.1140,   3.1140,   2.6394,  ...,   1.0853,   1.2636,   1.2636],\n",
      "         [  3.1140,   3.1140,   2.6394,  ...,   1.0853,   1.2636,   1.2636]],\n",
      "\n",
      "        [[ -2.4733,  -2.4733,  -2.5413,  ...,  -3.2516,  -3.1523,  -3.1523],\n",
      "         [ -2.4733,  -2.4733,  -2.5413,  ...,  -3.2516,  -3.1523,  -3.1523],\n",
      "         [ -2.5182,  -2.5182,  -2.5511,  ...,  -3.1761,  -3.0919,  -3.0919],\n",
      "         ...,\n",
      "         [ -4.2203,  -4.2203,  -4.1975,  ...,  -4.2061,  -4.1547,  -4.1547],\n",
      "         [ -4.2090,  -4.2090,  -4.1810,  ...,  -4.1577,  -4.1133,  -4.1133],\n",
      "         [ -4.2090,  -4.2090,  -4.1810,  ...,  -4.1577,  -4.1133,  -4.1133]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch2trt\n",
    "from torchvision import models\n",
    "\n",
    "# Ensure your model architecture is defined and moved to GPU\n",
    "model = ICNet(num_classes=19).cuda().eval()\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('model_state_dict.pth'))\n",
    "\n",
    "# Create dummy input data as a tensor on the GPU\n",
    "x = torch.randn(1, 3, 256, 512).cuda()  # Adjust input size as necessary\n",
    "\n",
    "# Convert to TensorRT using torch2trt\n",
    "model_trt = torch2trt.torch2trt(model, [x])\n",
    "\n",
    "# Create a non-default CUDA stream\n",
    "stream = torch.cuda.Stream()\n",
    "\n",
    "# Perform inference using the non-default stream\n",
    "with torch.cuda.stream(stream):\n",
    "    output = model_trt(x)\n",
    "\n",
    "# Ensure the stream is synchronized before accessing the results\n",
    "stream.synchronize()\n",
    "\n",
    "print(\"Output Shape:\", output.shape)\n",
    "print(\"Sample Output:\", output[0, :5])  # Print part of the output to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Inference Time: 0.002998 seconds\n",
      "[07/16/2024-03:16:17] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "TensorRT Inference Time: 0.000863 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Create a dummy input tensor\n",
    "x = torch.randn(1, 3, 256, 512).cuda()\n",
    "\n",
    "# Timing the PyTorch model\n",
    "start_time = time.time()\n",
    "output = model(x)\n",
    "end_time = time.time()\n",
    "pytorch_inference_time = end_time - start_time\n",
    "print(f\"PyTorch Inference Time: {pytorch_inference_time:.6f} seconds\")\n",
    "\n",
    "# Timing the TensorRT model\n",
    "start_time = time.time()\n",
    "output_trt = model_trt(x)\n",
    "end_time = time.time()\n",
    "tensorrt_inference_time = end_time - start_time\n",
    "print(f\"TensorRT Inference Time: {tensorrt_inference_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enet_cityscapes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
